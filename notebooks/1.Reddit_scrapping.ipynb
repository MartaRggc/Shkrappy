{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a606a8e6",
   "metadata": {},
   "source": [
    "# Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "787c9628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import datetime\n",
    "from joblib import Parallel, delayed\n",
    "import warnings\n",
    "import re\n",
    "import requests\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sys.path.append('/home/marta/Desktop/IH/Shkrappy/src')\n",
    "from fun import * \n",
    "\n",
    "\n",
    "# Establecemos la ruta al driver:\n",
    "\n",
    "PATH = 'driver/chromedriver'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f42321",
   "metadata": {},
   "source": [
    "# Funciones de scrapeo\n",
    "\n",
    "Vamos a diseñar las distintas funciones que usaremos en el scrapeo de reddit:\n",
    "\n",
    "+ extract: Una función que reciba un post concreto y extraiga la información relevante, es decir, el títlo del post, la fecha y, si es un meme, la url de la imagen asociada.\n",
    "+ scrappy: Una función que vaya scrolleando el subreddit en cuestión y que luego llame a extract para extraer la información de todos los posts que localice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67e914f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(post):\n",
    "    \n",
    "    try:\n",
    "        date = post.get_attribute('created-timestamp').split('T') # Partimos la fecha por la T\n",
    "        time = pd.to_datetime(date[1].split('.')[0]).time() # La segunda parte es la hora UTC\n",
    "        day = pd.to_datetime(date[0]).date() # La segunda parte es la fecha\n",
    "        title = post.get_attribute('post-title') # Sacamos el título del post\n",
    "        \n",
    "        try: # Esta parte es para obtener la url de los posts que tengan una imagen \n",
    "            link = post.find_elements(By.TAG_NAME, 'faceplate-img')[1].get_attribute('src')\n",
    "            return [day,time,title,link]\n",
    "        except:\n",
    "            return [day,time,title] # Por si no hay imagen\n",
    "    except:\n",
    "        print('Oups! Something went wrong.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17e2eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrappy(url):\n",
    "    \n",
    "    # Establecemos unos índices para el scrolleo contando el número de posts que podemos ver en la página:\n",
    "    \n",
    "    oldlen = 0\n",
    "    newlen = 1\n",
    "    newposts = newlen > oldlen \n",
    "    \n",
    "    # Abrimos el navegador y buscamos los posts:\n",
    "    \n",
    "    driver = webdriver.Chrome(PATH)    \n",
    "    driver.get(url)    \n",
    "    posts = driver.find_elements(By.TAG_NAME, 'shreddit-post') # Busca los posts\n",
    "    \n",
    "    # Scrolling todo lo que podamos, hasta que no cambie el último post que encontremos:\n",
    "\n",
    "    while newposts: \n",
    "        oldlen = newlen # Guardamos los posts que podemos ver ahora.\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") # Scrolleamos hasta abajo.\n",
    "        time.sleep(3) # Esperamos a que cargue.\n",
    "        posts = driver.find_elements(By.TAG_NAME, 'shreddit-post') # Buscamos de nuevo los posts.\n",
    "        newlen = len(posts) # Contamos cuantos hay.\n",
    "        newposts = newlen > oldlen # Miramos si hay los mismos que antes de scrollear, y si no repetimos proceso.\n",
    "                 \n",
    "    print('Scroll completed.') \n",
    "    \n",
    "    # Ahora usamos extract para extraer los datos de los posts totales que hemos conseguido.\n",
    "    # Los guardamos en un dataframe, le ponemos nombre a las columnas y cerramos el navegador:\n",
    "    \n",
    "    data = pd.DataFrame([extract(i) for i in posts])\n",
    "    \n",
    "    try:\n",
    "        data.columns = ['date','time','title']\n",
    "    except:\n",
    "        data.columns = ['date','time','title','link'] \n",
    "        \n",
    "        \n",
    "    driver.quit()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fb7e31",
   "metadata": {},
   "source": [
    "# URLs de Reddit a scrapear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f209f86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.reddit.com/r/memes/new/?feedViewType=classicView',\n",
       " 'https://www.reddit.com/r/programming/new/?feedViewType=classicView',\n",
       " 'https://www.reddit.com/r/science/new/?feedViewType=classicView',\n",
       " 'https://www.reddit.com/r/relationships/new/?feedViewType=classicView',\n",
       " 'https://www.reddit.com/r/usa/new/?feedViewType=classicView',\n",
       " 'https://www.reddit.com/r/canada/new/?feedViewType=classicView',\n",
       " 'https://www.reddit.com/r/unitedkingdom/new/?feedViewType=classicView']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hemos elegido 7 subforos. Construimos las url a partir de la principal y elegimos la vista que sabemos\n",
    "# scrapear con las funciones anteriores:\n",
    "\n",
    "main_url = 'https://www.reddit.com/r/'\n",
    "subredd = ['memes','programming','science','relationships','usa','canada','unitedkingdom']\n",
    "view ='/new/?feedViewType=classicView'\n",
    "\n",
    "all_url = [main_url+i+view for i in subredd]\n",
    "\n",
    "all_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a3a38",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72cc5e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "/tmp/ipykernel_5179/1601430830.py:11: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "/tmp/ipykernel_5179/1601430830.py:11: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scroll completed.\n",
      "CPU times: user 52 ms, sys: 31.2 ms, total: 83.2 ms\n",
      "Wall time: 11min 11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed: 11.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=2)]: Done   2 out of   2 | elapsed: 11.2min finished\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Paralelizamos el proceso para poder scrapear todas las páginas a la vez:\n",
    "\n",
    "parelelo = Parallel(n_jobs = 7, verbose = True)\n",
    "lst_df = parelelo(delayed(scrappy)(u) for u in all_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf4d663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hemos obtenido una lista de dataframes, uno por cada subreddit scrapeado. Echémosles un ojo:\n",
    "\n",
    "memes = lst_df[0]\n",
    "programming = lst_df[1]\n",
    "science = lst_df[2]\n",
    "relationships = lst_df[3]\n",
    "usa = lst_df[4]\n",
    "canada = lst_df[5]\n",
    "uk = lst_df[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4017db6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(432, 383, 450, 465, 333, 479, 471)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cada uno tiene un número de filas distinto, porque el scrolleo máximo permitido por reddit \n",
    "# parece variar según el foro.\n",
    "\n",
    "memes.shape[0], programming.shape[0], science.shape[0], relationships.shape[0], usa.shape[0], canada.shape[0], uk.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9301107b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-05</td>\n",
       "      <td>18:41:57</td>\n",
       "      <td>It’s not a donkey, but it’s sooooooo close!</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/UV_IXPBxkdRIy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-05</td>\n",
       "      <td>18:25:56</td>\n",
       "      <td>i mean, it's that simple, right?[Shrek]</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/rIylb4UOJytxk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-08-05</td>\n",
       "      <td>18:24:00</td>\n",
       "      <td>I have no idea what in the shrek is going on.</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/kZYq6Q008yFx0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-08-05</td>\n",
       "      <td>18:14:20</td>\n",
       "      <td>This could be us Donkey, but I don't want it to.</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/YHlalXXkYDcjf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-08-05</td>\n",
       "      <td>17:48:25</td>\n",
       "      <td>You see a fairy everywhere</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/AjrIhjiUijUmx...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      time                                             title  \\\n",
       "0  2023-08-05  18:41:57       It’s not a donkey, but it’s sooooooo close!   \n",
       "1  2023-08-05  18:25:56           i mean, it's that simple, right?[Shrek]   \n",
       "2  2023-08-05  18:24:00     I have no idea what in the shrek is going on.   \n",
       "3  2023-08-05  18:14:20  This could be us Donkey, but I don't want it to.   \n",
       "4  2023-08-05  17:48:25                        You see a fairy everywhere   \n",
       "\n",
       "                                                link  \n",
       "0  https://b.thumbs.redditmedia.com/UV_IXPBxkdRIy...  \n",
       "1  https://b.thumbs.redditmedia.com/rIylb4UOJytxk...  \n",
       "2  https://b.thumbs.redditmedia.com/kZYq6Q008yFx0...  \n",
       "3  https://b.thumbs.redditmedia.com/YHlalXXkYDcjf...  \n",
       "4  https://b.thumbs.redditmedia.com/AjrIhjiUijUmx...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "21c6967b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-08-05</td>\n",
       "      <td>18:10:03</td>\n",
       "      <td>Node.js vs Reactive Java</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/mDYcAK-jpWnIw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-08-05</td>\n",
       "      <td>17:11:43</td>\n",
       "      <td>Turing's Maze</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-08-05</td>\n",
       "      <td>17:03:54</td>\n",
       "      <td>Show Notifications in your Terminal</td>\n",
       "      <td>https://a.thumbs.redditmedia.com/LsNWQlGzD2Hfg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-08-05</td>\n",
       "      <td>16:25:16</td>\n",
       "      <td>Software engineering diploma advice</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-08-05</td>\n",
       "      <td>16:15:17</td>\n",
       "      <td>What is UNIX: An Immersion in Bell Laboratorie...</td>\n",
       "      <td>https://b.thumbs.redditmedia.com/3iHyf-3CAMMk5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date      time                                              title  \\\n",
       "0  2023-08-05  18:10:03                           Node.js vs Reactive Java   \n",
       "1  2023-08-05  17:11:43                                      Turing's Maze   \n",
       "2  2023-08-05  17:03:54                Show Notifications in your Terminal   \n",
       "3  2023-08-05  16:25:16                Software engineering diploma advice   \n",
       "4  2023-08-05  16:15:17  What is UNIX: An Immersion in Bell Laboratorie...   \n",
       "\n",
       "                                                link  \n",
       "0  https://b.thumbs.redditmedia.com/mDYcAK-jpWnIw...  \n",
       "1                                               None  \n",
       "2  https://a.thumbs.redditmedia.com/LsNWQlGzD2Hfg...  \n",
       "3                                               None  \n",
       "4  https://b.thumbs.redditmedia.com/3iHyf-3CAMMk5...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "programming.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e1ace",
   "metadata": {},
   "source": [
    "# Guardado de datos\n",
    "\n",
    "Parece que están bien. Ya vemos que algunos tienen nulos en la columna de links, porque no todos los posts tenían imágenes asociadas. En el dataframe de relationships no hay esa columna porque directamente no había ninguna imagen en ninguno de los posts. \n",
    "\n",
    "Guardémoslos en csv. y ya rellenaremos los nulos en el siguiente dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17967355",
   "metadata": {},
   "outputs": [],
   "source": [
    "memes.to_csv('../data/raw_data/reddit/memes.csv', index = False)\n",
    "programming.to_csv('../data/raw_data/reddit/programming.csv', index = False)\n",
    "science.to_csv('../data/raw_data/reddit/science.csv', index = False)\n",
    "relationships.to_csv('../data/raw_data/reddit/relationships.csv', index = False)\n",
    "usa.to_csv('../data/raw_data/reddit/usa.csv', index = False)\n",
    "canada.to_csv('../data/raw_data/reddit/canada.csv', index = False)\n",
    "uk.to_csv('../data/raw_data/reddit/uk.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
